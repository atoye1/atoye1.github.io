# 1인 기업가 지식 자동화 시스템 설계 기획안

## 개요

### 시스템 목적
- 정보의 자동 수집, 분류, 가공을 통한 지적 산출물 생성 자동화
- "강물에서 금 채취"하듯 행동 가능한 가치 있는 정보 필터링
- 1인 기업가의 지식 노동 생산성 극대화

### 핵심 철학
- **행동 지향성**: 실제 수익이나 성장으로 이어질 수 있는 정보 우선
- **시간축 분류**: Hot(즉시) / Warm(단기) / Cold(장기) 정보 구분
- **이중 구조**: 시간순 수집 → 토픽별 재정리 → 링크 참조 체계

## 시스템 아키텍처

### 전체 구조
```
정보 소스들 → [수집기] → [정리기] → [처리기] → 산출물들
     ↓            ↓         ↓         ↓
  다양한 채널   → 날짜별 수집 → 토픽별 분류 → 자동 생성
```

## 1. 정보 수집기 (Data Collector)

### 1.1 수집 대상
- **정부 정책 공지사항**: 행정안전부, 각 부처 RSS/API
- **문화행사**: 문화체육관광부, 지역 문화재단 공지
- **AI 뉴스레터**: 구독 중인 뉴스레터들
- **미디움 블로그**: 관심 작가들의 새 글
- **유튜브 채널**: 관심 채널의 새 영상
- **개인 노트**: 기존 텔레그램 → 깃허브 워크플로우

### 1.2 수집 도구별 역할

#### Airflow (주기적 크롤러)
- **일일 배치**: 정부 사이트, 뉴스 사이트 크롤링
- **주간 배치**: 유튜브 API를 통한 새 영상 수집
- **월간 배치**: 미디움, 블로그 RSS 수집

#### n8n (실시간/이벤트 기반)
- **텔레그램 봇**: 즉석 노트, 북마크 처리 (기존 워크플로우 확장)
- **웹훅**: IFTTT/Zapier 연동으로 새 북마크 자동 수집
- **이메일 모니터링**: 뉴스레터 자동 파싱

### 1.3 수집 데이터 표준화
```json
{
  "id": "unique_identifier",
  "timestamp": "2024-01-15T09:30:00Z",
  "source_type": "government|culture|ai_news|medium|youtube|personal",
  "source_url": "원본 URL",
  "title": "제목",
  "content": "본문 또는 요약",
  "raw_content": "원본 전체 텍스트",
  "metadata": {
    "deadline": "2024-02-15",  // 있는 경우만
    "author": "작성자",
    "tags": ["자동생성태그1", "자동생성태그2"]
  }
}
```

## 2. 정보 정리기 (Data Organizer)

### 2.1 1차 정리 (날짜별 수집)
- **저장 구조**: `/daily/YYYY/MM/DD-daily.md`
- **처리 방식**: 수집된 모든 정보를 시간순으로 append
- **자동 태깅**: LLM을 활용한 초기 분류 및 태그 생성

### 2.2 2차 정리 (토픽별 재분류)
#### 일일 루틴 프로세스
1. **전날 수집 데이터 분석**: LLM으로 토픽 추출
2. **기존 카테고리 매칭**: 미리 정의된 카테고리와 비교
3. **새 토픽 생성**: 기존에 없는 경우 새 카테고리 생성
4. **우선순위 분류**:
   - **Hot**: 마감 7일 이내, 즉시 행동 필요
   - **Warm**: 1-3개월 내 활용 예정
   - **Cold**: 장기 학습, 참조용

#### 저장 구조
```
/topics/
  ├── government_policy/
  │   ├── startup_support/
  │   └── ai_regulation/
  ├── ai_trends/
  │   ├── tools/
  │   └── research/
  └── business_insights/
      ├── marketing/
      └── productivity/
```

### 2.3 링크 참조 시스템
- **원본 참조**: `[정책명](../daily/2024/01/15-daily.md#정부정책)`
- **소스 트레이싱**: 각 정보의 출처 URL과 수집 시점 기록
- **연관 정보**: NLP 유사도 기반 관련 정보 자동 링크

## 3. 정보 처리기 (Content Generator)

### 3.1 산출물 생성 프로세스

#### 일간 처리
- **Hot 정보 알림**: 즉시 행동이 필요한 정보 요약
- **일일 인사이트**: 수집된 정보 중 흥미로운 연결점 발견

#### 주간 처리  
- **주간 트렌드 분석**: 반복 출현하는 키워드/토픽 분석
- **SNS 콘텐츠 생성**: 트위터 스레드, 링크드인 포스트

#### 월간 처리
- **깊이 있는 분석**: 블로그 포스트, 뉴스레터
- **프로젝트 아이디어**: 수집된 정보 기반 새 프로젝트 제안

### 3.2 LLM 활용 전략
- **컨텍스트 구성**: 관련 토픽의 과거 정보 자동 수집
- **스타일 일관성**: 개인 글쓰기 스타일 학습 및 적용
- **출처 명시**: 생성된 내용의 근거가 된 원본 정보 링크

### 3.3 산출물 유형별 템플릿

#### 블로그 포스트
```markdown
# 제목 (트렌드 키워드 기반)
## 배경 (수집된 정보 요약)
## 분석 (개인 인사이트)
## 실행 방안 (행동 가능한 제안)
## 참고 자료 (출처 링크들)
```

#### 트위터 스레드
```
1/ 오늘 발견한 흥미로운 패턴 (Hook)
2/ 데이터 1: [정부 정책 요약]
3/ 데이터 2: [업계 동향 요약]  
4/ 인사이트: [연결점 분석]
5/ 실행 팁: [구체적 행동 방안]
```

## 4. 기술 구현 세부사항

### 4.1 n8n 워크플로우 확장

#### 기존 워크플로우 개선
- **텔레그램 봇 고도화**: 명령어별 다른 처리 로직
  - `/note`: 일반 노트 (기존)
  - `/bookmark`: URL 북마크 + 자동 요약
  - `/idea`: 아이디어 노트 + 우선순위 태깅

#### 새 워크플로우 추가
1. **RSS/API 수집 플로우**
   - 스케줄 트리거 → API 호출 → 데이터 정규화 → 깃허브 커밋

2. **일일 정리 플로우**
   - 전날 데이터 수집 → LLM 분석 → 토픽별 분류 → 파일 생성

3. **콘텐츠 생성 플로우**
   - 트리거(수동/스케줄) → 관련 데이터 수집 → LLM 생성 → 플랫폼별 배포

### 4.2 LLM 통합
- **모델 선택**: GPT-4 또는 Claude (API 기반)
- **프롬프트 엔지니어링**: 
  - 분류용 프롬프트
  - 요약용 프롬프트  
  - 콘텐츠 생성용 프롬프트
- **컨텍스트 관리**: RAG 방식으로 관련 정보 주입

### 4.3 데이터 저장소
- **주 저장소**: GitHub Repository (마크다운 파일)
- **검색 인덱스**: 로컬 데이터베이스 (SQLite) 또는 벡터 DB
- **백업**: 클라우드 스토리지 자동 동기화

## 5. 구현 단계별 로드맵

### Phase 1: 기반 확장 (1-2주)
- [ ] 기존 텔레그램 워크플로우 명령어 확장
- [ ] 간단한 RSS 수집 워크플로우 구축
- [ ] 기본 폴더 구조 및 파일 템플릿 설정

### Phase 2: 자동 분류 (2-3주)  
- [ ] LLM 기반 토픽 분류 시스템 구축
- [ ] 우선순위(Hot/Warm/Cold) 자동 판단 로직
- [ ] 일일 정리 루틴 자동화

### Phase 3: 콘텐츠 생성 (3-4주)
- [ ] 기본 산출물 템플릿 작성
- [ ] LLM 기반 콘텐츠 생성 워크플로우
- [ ] 플랫폼별 배포 자동화

### Phase 4: 고도화 (지속)
- [ ] NLP 유사도 기반 연관 정보 추천
- [ ] 개인화된 인사이트 패턴 학습
- [ ] 성과 측정 및 시스템 개선

## 6. 성공 지표

### 정량적 지표
- **처리 효율성**: 정보 수집→산출물 생성 시간 단축
- **산출물 품질**: 생성된 콘텐츠의 참여도/반응
- **정보 활용률**: 수집 정보 중 실제 활용된 비율

### 정성적 지표  
- **인사이트 품질**: 기존에 놓쳤을 연결점 발견
- **행동 변화**: 실제 의사결정에 영향을 준 정보 사례
- **시간 절약**: 반복 작업 자동화를 통한 시간 확보

## 7. 위험 요소 및 대응책

### 기술적 위험
- **API 제한**: 백업 소스 및 캐싱 전략 수립
- **LLM 비용**: 효율적인 프롬프트 및 사용량 모니터링
- **데이터 유실**: 다중 백업 시스템 구축

### 운영적 위험
- **정보 과부하**: 필터링 기준 지속적 개선
- **품질 저하**: 인간 검증 단계 유지
- **의존성**: 수동 처리 능력 병행 유지

---

이 시스템을 통해 정보의 강물에서 지속적으로 가치를 채취하고, 이를 체계적으로 가공하여 의미 있는 지적 산출물로 전환하는 자동화된 지식 생태계를 구축할 수 있습니다.